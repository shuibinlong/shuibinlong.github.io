---
title: 什么是知识蒸馏
date: 2021-12-30 15:01:43
tags:
---

### 什么是蒸馏

为了提高模型的效果。常用两种做法：
- 使用更复杂的模型，例如从`TextCNN`到`BERT`。
- 使用多个简单模型集成。

当不涉及实时性的要求，模型在离线训练时没有问题。但一旦涉及到部署模型，线上进行实时推理时候，需要考虑时延和计算资源，需要在模型复杂度和模型精度之间做一个平衡。这个时候，将我们从大模型学到的信息提取精华灌输到小模型去，这个过程称作蒸馏。

### 什么是知识

对于一个模型，算法工程师一般关注两部分：
- 模型架构
- 模型参数

这两个部分可以当作是模型从数据中学习到的信息和知识（当然我们主要关注参数，因为模型架构一般在训练之前就已经定下来了）。但深度学习领域里，这两部分都属于黑箱，我们不知道里面究竟发生了什么事情。而肉眼可以观测到的是模型的输入和输出，即从一个向量（或数值）经过黑盒运算得到另一个向量（或数值），例如在分类任务中，输入标签形如$[0,0,1,0]$，输出形如$[0.01,0.01,0.97,0.01]$，好比输入图像是一辆宝马车，那么模型再宝马车这个类别上会有最大的概率值，与此同时还会把剩余的概率值分配给其他类别，但这些类别概率值一般比较小，例如把垃圾车的概率就会比胡萝卜的概率更高一些。

当模型的输出结果含有的信息更丰富以后，它的信息熵也更大，进一步就可以把它当作一种知识，也就是小模型需要从大模型中学习到的经验。我们一般把这种更复杂的大模型称之为老师网络，更简单的需要蒸馏的模型称之为学生网络，学生网络通过学习老师网络的输出进而训练模型，达到比较好的收敛效果。